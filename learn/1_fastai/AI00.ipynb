{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SOLEROM/tinyai/blob/main/AI00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM0EtmDuBRcb"
   },
   "source": [
    "ABOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zw1ong7_7aCU"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true,
    "id": "NZHP6nCMBG8R",
    "outputId": "e021bf7b-5f4b-4081-da01-d33a378f03b8"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"489pt\" height=\"134pt\"\n",
       " viewBox=\"0.00 0.00 489.18 134.36\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 130.36)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-130.36 485.18,-130.36 485.18,4 -4,4\"/>\n",
       "<!-- model -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"217.09,-79.36 141.09,-79.36 137.09,-75.36 137.09,-29.36 213.09,-29.36 217.09,-33.36 217.09,-79.36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"213.09,-75.36 137.09,-75.36 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"213.09,-75.36 213.09,-29.36 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"213.09,-75.36 217.09,-79.36 \"/>\n",
       "<text text-anchor=\"middle\" x=\"177.09\" y=\"-50.66\" font-family=\"Times,serif\" font-size=\"14.00\">architecture</text>\n",
       "</g>\n",
       "<!-- predictions -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>predictions</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"340.14\" cy=\"-54.36\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"340.14\" y=\"-50.66\" font-family=\"Times,serif\" font-size=\"14.00\">predictions</text>\n",
       "</g>\n",
       "<!-- model&#45;&gt;predictions -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>model&#45;&gt;predictions</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.49,-54.36C236.29,-54.36 259.19,-54.36 280.02,-54.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.06,-57.86 290.06,-54.36 280.06,-50.86 280.06,-57.86\"/>\n",
       "</g>\n",
       "<!-- inputs -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>inputs</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"50.05\" cy=\"-74.36\" rx=\"32.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.05\" y=\"-70.66\" font-family=\"Times,serif\" font-size=\"14.00\">inputs</text>\n",
       "</g>\n",
       "<!-- inputs&#45;&gt;model -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>inputs&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M81.64,-69.47C95.15,-67.31 111.38,-64.71 126.54,-62.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"127.51,-65.67 136.83,-60.64 126.4,-58.76 127.51,-65.67\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"454.18\" cy=\"-83.36\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"454.18\" y=\"-79.66\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predictions&#45;&gt;loss -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>predictions&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M381.27,-64.75C393.51,-67.91 406.85,-71.37 418.68,-74.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"418.07,-77.89 428.63,-77 419.82,-71.11 418.07,-77.89\"/>\n",
       "</g>\n",
       "<!-- parameters -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>parameters</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"50.05\" cy=\"-20.36\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.05\" y=\"-16.66\" font-family=\"Times,serif\" font-size=\"14.00\">parameters</text>\n",
       "</g>\n",
       "<!-- parameters&#45;&gt;model -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>parameters&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90.61,-31.12C102.13,-34.25 114.85,-37.71 126.88,-40.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"126.22,-44.42 136.78,-43.67 128.05,-37.67 126.22,-44.42\"/>\n",
       "</g>\n",
       "<!-- labels -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>labels</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"340.14\" cy=\"-108.36\" rx=\"31.4\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"340.14\" y=\"-104.66\" font-family=\"Times,serif\" font-size=\"14.00\">labels</text>\n",
       "</g>\n",
       "<!-- labels&#45;&gt;loss -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>labels&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M369.41,-102.05C384.3,-98.73 402.69,-94.63 418.44,-91.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"419.35,-94.49 428.35,-88.9 417.83,-87.66 419.35,-94.49\"/>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;parameters -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>loss&#45;&gt;parameters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M440.88,-67.53C429.39,-54.1 410.95,-35.74 390.18,-27.36 295.38,10.89 173.21,0.49 104.38,-10.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"103.53,-6.77 94.21,-11.81 104.64,-13.68 103.53,-6.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"253.59\" y=\"-6.16\" font-family=\"Times,serif\" font-size=\"14.00\">update</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7fbbb622ec40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "#caption Detailed training loop\n",
    "#id detailed_loop\n",
    "from fastbook import *\n",
    "\n",
    "gv('''ordering=in\n",
    "model[shape=box3d width=1 height=0.7 label=architecture]\n",
    "inputs->model->predictions; parameters->model; labels->loss; predictions->loss\n",
    "loss->parameters[constraint=false label=update]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uEWdx7nBG8d"
   },
   "source": [
    "# Jargon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L0NfeIjBG8e"
   },
   "source": [
    "\n",
    "* Label | The data that we're trying to predict, such as \"dog\" or \"cat\"\n",
    "* Architecture | The _template_ of the model that we're trying to fit; the actual mathematical function that we're passing the input data and parameters to\n",
    "* Model | The combination of the architecture with a particular set of parameters\n",
    "* Parameters | The values in the model that change what task it can do, and are updated through model training\n",
    "* Fit | Update the parameters of the model such that the predictions of the model using the input data match the target labels\n",
    "* Train | A synonym for _fit_\n",
    "* Pretrained model | A model that has already been trained, generally using a large dataset, and will be fine-tuned\n",
    "* Fine-tune | Update a pretrained model for a different task\n",
    "* Epoch | One complete pass through the input data\n",
    "* Loss | A measure of how good the model is, chosen to drive training via SGD\n",
    "* Metric | A measurement of how good the model is, using the validation set, chosen for human consumption\n",
    "* Validation set | A set of data held out from training, used only for measuring how good the model is\n",
    "* Training set | The data used for fitting the model; does not include any data from the validation set\n",
    "* Overfitting | Training a model in such a way that it _remembers_ specific features of the input data, rather than generalizing well to data not seen during training\n",
    "* CNN | Convolutional neural network; a type of neural network that works particularly well for computer vision tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAw_uUYnBG8Z"
   },
   "source": [
    "# What Image Recognizer Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UMCgO3BBG8Z"
   },
   "source": [
    "At this stage we have an image recognizer that is working very well, but we have no idea what it is actually doing! Although many people complain that deep learning results in impenetrable \"black box\" models (that is, something that gives predictions but that no one can understand), this really couldn't be further from the truth. There is a vast body of research showing how to deeply inspect deep learning models, and get rich insights from them. Having said that, all kinds of machine learning models (including deep learning, and traditional statistical models) can be challenging to fully understand, especially when considering how they will behave when coming across data that is very different to the data used to train them. We'll be discussing this issue throughout this book.\n",
    "\n",
    "In 2013 a PhD student, Matt Zeiler, and his supervisor, Rob Fergus, published the paper [\"Visualizing and Understanding Convolutional Networks\"](https://arxiv.org/pdf/1311.2901.pdf), which showed how to visualize the neural network weights learned in each layer of a model. They carefully analyzed the model that won the 2012 ImageNet competition, and used this analysis to greatly improve the model, such that they were able to go on to win the 2013 competition! <<img_layer1>> is the picture that they published of the first layer's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fszSKQyBG8Z"
   },
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/layer1.png?raw=1\" alt=\"Activations of the first layer of a CNN\" width=\"300\" caption=\"Activations of the first layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7f5xgOGBG8Z"
   },
   "source": [
    "This picture requires some explanation. For each layer, the image part with the light gray background shows the reconstructed weights pictures, and the larger section at the bottom shows the parts of the training images that most strongly matched each set of weights. For layer 1, what we can see is that the model has discovered weights that represent diagonal, horizontal, and vertical edges, as well as various different gradients. (Note that for each layer only a subset of the features are shown; in practice there are thousands across all of the layers.) These are the basic building blocks that the model has learned for computer vision. They have been widely analyzed by neuroscientists and computer vision researchers, and it turns out that these learned building blocks are very similar to the basic visual machinery in the human eye, as well as the handcrafted computer vision features that were developed prior to the days of deep learning. The next layer is represented in <<img_layer2>>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DK8o1EL6BG8Z"
   },
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/layer2.png?raw=1\" alt=\"Activations of the second layer of a CNN\" width=\"800\" caption=\"Activations of the second layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd_-lhbrBG8Z"
   },
   "source": [
    "For layer 2, there are nine examples of weight reconstructions for each of the features found by the model. We can see that the model has learned to create feature detectors that look for corners, repeating lines, circles, and other simple patterns. These are built from the basic building blocks developed in the first layer. For each of these, the right-hand side of the picture shows small patches from actual images which these features most closely match. For instance, the particular pattern in row 2, column 1 matches the gradients and textures associated with sunsets.\n",
    "\n",
    "<<img_layer3>> shows the image from the paper showing the results of reconstructing the features of layer 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmiZsNCpBG8a"
   },
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter2_layer3.PNG?raw=1\" alt=\"Activations of the third layer of a CNN\" width=\"800\" caption=\"Activations of the third layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "422MjPOVBG8a"
   },
   "source": [
    "As you can see by looking at the righthand side of this picture, the features are now able to identify and match with higher-level semantic components, such as car wheels, text, and flower petals. Using these components, layers four and five can identify even higher-level concepts, as shown in <<img_layer4>>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVeQQMYnBG8a"
   },
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter2_layer4and5.PNG?raw=1\" alt=\"Activations of layers 4 and 5 of a CNN\" width=\"800\" caption=\"Activations of layers 4 and 5 of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9NHz3WiBG8a"
   },
   "source": [
    "This article was studying an older model called *AlexNet* that only contained five layers. Networks developed since then can have hundreds of layers—so you can imagine how rich the features developed by these models can be! \n",
    "\n",
    "When we fine-tuned our pretrained model earlier, we adapted what those last layers focus on (flowers, humans, animals) to specialize on the cats versus dogs problem. More generally, we could specialize such a pretrained model on many different tasks. Let's have a look at some examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In machine learning and deep learning, we can’t do anything without data\n",
    "\n",
    "often start by using one of the cut-down versions and later scale up to the full-size version (just as we're doing in this chapter!). In fact, this is how the world’s top practitioners do their modeling in practice; they do most of their experimentation and prototyping with subsets of their data, and only use the full dataset when they have a good understanding of what they have to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validaion set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " split our dataset into two sets: the training set (which our model sees in training) and the validation set, also known as the development set (which is used only for evaluation). This lets us test that the model learns lessons from the training data that generalize to new data, the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we must hold back the test set data even from ourselves. It cannot be used to improve the model; it can only be used to evaluate the model at the very end of our efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNMovFOTJ5FtqVKmWH3xh0W",
   "include_colab_link": true,
   "name": "AI00.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
