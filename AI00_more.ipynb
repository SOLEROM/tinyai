{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Copy of 01_intro.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SOLEROM/tinyai/blob/main/AI00_more.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6NUBB3KBG8a"
      },
      "source": [
        "### Image Recognizers Can Tackle Non-Image Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMbTvrD_BG8b"
      },
      "source": [
        "An image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recogniser can learn to complete many tasks.\n",
        "\n",
        "For instance, a sound can be converted to a spectrogram, which is a chart that shows the amount of each frequency at each time in an audio file. Fast.ai student Ethan Sutin used this approach to easily beat the published accuracy of a state-of-the-art [environmental sound detection model](https://medium.com/@etown/great-results-on-audio-classification-with-fastai-library-ccaf906c5f52) using a dataset of 8,732 urban sounds. fastai's `show_batch` clearly shows how each different sound has a quite distinctive spectrogram, as you can see in <<img_spect>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdPiX6OuBG8b"
      },
      "source": [
        "<img alt=\"show_batch with spectrograms of sounds\" width=\"400\" caption=\"show_batch with spectrograms of sounds\" id=\"img_spect\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00012.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUFES_x0BG8b"
      },
      "source": [
        "A time series can easily be converted into an image by simply plotting the time series on a graph. However, it is often a good idea to try to represent your data in a way that makes it as easy as possible to pull out the most important components. In a time series, things like seasonality and anomalies are most likely to be of interest. There are various transformations available for time series data. For instance, fast.ai student Ignacio Oguiza created images from a time series dataset for olive oil classification, using a technique called Gramian Angular Difference Field (GADF); you can see the result in <<ts_image>>. He then fed those images to an image classification model just like the one you see in this chapter. His results, despite having only 30 training set images, were well over 90% accurate, and close to the state of the art."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e8Cc19lBG8c"
      },
      "source": [
        "<img alt=\"Converting a time series into an image\" width=\"700\" caption=\"Converting a time series into an image\" id=\"ts_image\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00013.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Wr_4dqBG8c"
      },
      "source": [
        "Another interesting fast.ai student project example comes from Gleb Esman. He was working on fraud detection at Splunk, using a dataset of users' mouse movements and mouse clicks. He turned these into pictures by drawing an image where the position, speed, and acceleration of the mouse pointer was displayed using coloured lines, and the clicks were displayed using [small colored circles](https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html), as shown in <<splunk>>. He then fed this into an image recognition model just like the one we've used in this chapter, and it worked so well that it led to a patent for this approach to fraud analytics!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IBHuk1zBG8c"
      },
      "source": [
        "<img alt=\"Converting computer mouse behavior to an image\" width=\"450\" caption=\"Converting computer mouse behavior to an image\" id=\"splunk\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00014.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5f1m70PBG8c"
      },
      "source": [
        "Another example comes from the paper [\"Malware Classification with Deep Convolutional Neural Networks\"](https://ieeexplore.ieee.org/abstract/document/8328749) by Mahmoud Kalash et al., which explains that \"the malware binary file is divided into 8-bit sequences which are then converted to equivalent decimal values. This decimal vector is reshaped and a gray-scale image is generated that represents the malware sample,\" like in <<malware_proc>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dojuh3EuBG8d"
      },
      "source": [
        "<img alt=\"Malware classification process\" width=\"623\" caption=\"Malware classification process\" id=\"malware_proc\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00055.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6pn-qtYBG8d"
      },
      "source": [
        "The authors then show \"pictures\" generated through this process of malware in different categories, as shown in <<malware_eg>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP0P6kNdBG8d"
      },
      "source": [
        "<img alt=\"Malware examples\" width=\"650\" caption=\"Malware examples\" id=\"malware_eg\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00056.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlReytbPBG8d"
      },
      "source": [
        "As you can see, the different types of malware look very distinctive to the human eye. The model the researchers trained based on this image representation was more accurate at malware classification than any previous approach shown in the academic literature. This suggests a good rule of thumb for converting a dataset into an image representation: if the human eye can recognize categories from the images, then a deep learning model should be able to do so too.\n",
        "\n",
        "In general, you'll find that a small number of general approaches in deep learning can go a long way, if you're a bit creative in how you represent your data! You shouldn't think of approaches like the ones described here as \"hacky workarounds,\" because actually they often (as here) beat previously state-of-the-art results. These really are the right ways to think about these problem domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm1LrUUYBG8e"
      },
      "source": [
        "## Deep Learning Is Not Just for Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juxGYUrvBG8e"
      },
      "source": [
        "Deep learning's effectiveness for classifying images has been widely discussed in recent years, even showing _superhuman_ results on complex tasks like recognizing malignant tumors in CT scans. But it can do a lot more than this, as we will show here.\n",
        "\n",
        "For instance, let's talk about something that is critically important for autonomous vehicles: localizing objects in a picture. If a self-driving car doesn't know where a pedestrian is, then it doesn't know how to avoid one! Creating a model that can recognize the content of every individual pixel in an image is called *segmentation*. Here is how we can train a segmentation model with fastai, using a subset of the [*Camvid* dataset](http://www0.cs.ucl.ac.uk/staff/G.Brostow/papers/Brostow_2009-PRL.pdf) from the paper \"Semantic Object Classes in Video: A High-Definition Ground Truth Database\" by Gabruel J. Brostow, Julien Fauqueur, and Roberto Cipolla:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9WNrLrGBG8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "7a926fa4-da01-4446-ccbc-9ff895a5ced9"
      },
      "source": [
        "path = untar_data(URLs.CAMVID_TINY)\n",
        "dls = SegmentationDataLoaders.from_label_func(\n",
        "    path, bs=8, fnames = get_image_files(path/\"images\"),\n",
        "    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n",
        "    codes = np.loadtxt(path/'codes.txt', dtype=str)\n",
        ")\n",
        "\n",
        "learn = unet_learner(dls, resnet34)\n",
        "learn.fine_tune(8)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b0aa8ba5c292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntar_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURLs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAMVID_TINY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m dls = SegmentationDataLoaders.from_label_func(\n\u001b[1;32m      3\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'{o.stem}_P{o.suffix}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'codes.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'untar_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqHxDcrHBG8f"
      },
      "source": [
        "We are not even going to walk through this code line by line, because it is nearly identical to our previous example! (Although we will be doing a deep dive into segmentation models in <<chapter_arch_details>>, along with all of the other models that we are briefly introducing in this chapter, and many, many more.)\n",
        "\n",
        "We can visualize how well it achieved its task, by asking the model to color-code each pixel of an image. As you can see, it nearly perfectly classifies every pixel in every object. For instance, notice that all of the cars are overlaid with the same color and all of the trees are overlaid with the same color (in each pair of images, the lefthand image is the ground truth label and the right is the prediction from the model):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ9HsxeyBG8f"
      },
      "source": [
        "learn.show_results(max_n=6, figsize=(7,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSIh_f0YBG8g"
      },
      "source": [
        "One other area where deep learning has dramatically improved in the last couple of years is natural language processing (NLP). Computers can now generate text, translate automatically from one language to another, analyze comments, label words in sentences, and much more. Here is all of the code necessary to train a model that can classify the sentiment of a movie review better than anything that existed in the world just five years ago:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9At4FKjBG8g"
      },
      "source": [
        "from fastai.text.all import *\n",
        "\n",
        "dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n",
        "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn.fine_tune(4, 1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}